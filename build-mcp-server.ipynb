{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Building MCP Servers for Compliance Documentation\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ethanolivertroy/pcidss-mcp-training/blob/main/build-mcp-server.ipynb)\n\n**A hands-on tutorial for GRC professionals**\n\n---\n\n## What You'll Learn\n\nThis notebook teaches you how to build a Model Context Protocol (MCP) server for compliance documentation. We use PCI-DSS as the example, but these patterns work for **any** compliance framework:\n\n- NIST 800-53\n- ISO 27001\n- SOC 2\n- FedRAMP\n- HIPAA\n\n### What is an MCP Server?\n\nMCP servers provide **tools** that AI assistants like Claude can call to:\n- Search structured compliance data\n- Retrieve specific requirements by ID\n- Map controls between frameworks\n- Generate compliance checklists\n\n### This Notebook is Fully Self-Contained\n\nEverything runs right here in **Python** using [FastMCP](https://github.com/jlowin/fastmcp) - the leading Python framework for MCP development. Works in:\n- Google Colab\n- Local Jupyter\n- VS Code with Colab extension\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## Design Principles for MCP Servers\n\nKey insights from [production MCP deployments](https://newsletter.pragmaticengineer.com/p/mcp-deepdive):\n\n### Design for Agents, Not Humans\nMCP servers are called by AI assistants, not humans directly:\n- Return structured JSON, not formatted text\n- Include metadata that helps agents understand context\n- Provide clear error messages with actionable suggestions\n\n### Start Internal, Then Public\nMost successful MCPs start as internal tools:\n- Validate use cases in controlled environments first\n- Security is easier when you control both client and server\n- Public servers face client compatibility challenges\n\n### Keep It Focused\n- Expose only necessary capabilities (least privilege)\n- One MCP per domain (compliance, ticketing, docs)\n- Avoid feature creep - simple tools are more reliable",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Source - PCI-DSS Prioritized Approach Tool\n",
    "\n",
    "### Why Excel Over PDF?\n",
    "\n",
    "Many compliance documents are available as PDFs, but PDFs are difficult to parse. The same data is often available in Excel format, which is:\n",
    "\n",
    "- **Already structured** - No complex extraction needed\n",
    "- **Reliable** - Consistent column layouts\n",
    "- **Fast** - Parse in milliseconds vs. minutes\n",
    "- **Accurate** - No OCR errors or layout issues\n",
    "\n",
    "For PCI-DSS, the **Prioritized Approach Tool** is an official Excel file from PCI Security Standards Council.\n",
    "\n",
    "### Download the Excel File\n",
    "\n",
    "1. Download from PCI SSC:\n",
    "   \n",
    "   **[Prioritized-Approach-Tool-For-PCI-DSS-v4_0_1.xlsx](https://docs-prv.pcisecuritystandards.org/PCI%20DSS/Supporting%20Document/Prioritized-Approach-Tool-For-PCI-DSS-v4_0_1.xlsx)**\n",
    "\n",
    "2. Accept the license agreement\n",
    "\n",
    "3. Upload when prompted (Colab) or place in your project directory (local)\n",
    "\n",
    "### Legal Notes\n",
    "\n",
    "The Excel file is licensed by PCI SSC for **internal use only**:\n",
    "- Use for your organization's compliance work\n",
    "- Do not redistribute converted data publicly\n",
    "- Each user should download and accept the license directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing Excel Data\n",
    "\n",
    "The Prioritized Approach Tool contains a sheet called \"Prioritized Approach Milestones\" with:\n",
    "\n",
    "| Column A | Column B |\n",
    "|----------|----------|\n",
    "| Requirement text | Milestone priority (1-6) |\n",
    "\n",
    "Requirements follow a hierarchical numbering:\n",
    "- **Level 1**: \"Requirement 1: Install and Maintain Network Security Controls\"\n",
    "- **Level 2**: \"1.1 Processes and mechanisms...\"\n",
    "- **Level 3**: \"1.1.1 All security policies...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q openpyxl\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "# Detect environment\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    print(\"\\nPlease upload the Prioritized Approach Tool Excel file...\")\n",
    "    print(\"Download from: https://docs-prv.pcisecuritystandards.org/PCI%20DSS/Supporting%20Document/Prioritized-Approach-Tool-For-PCI-DSS-v4_0_1.xlsx\")\n",
    "    uploaded = files.upload()\n",
    "    excel_path = Path(list(uploaded.keys())[0])\n",
    "    print(f\"\\nUploaded: {excel_path.name}\")\n",
    "else:\n",
    "    excel_path = Path(\"Prioritized-Approach-Tool-For-PCI-DSS-v4_0_1.xlsx\")\n",
    "    if not excel_path.exists():\n",
    "        print(f\"Excel file not found. Please download from:\")\n",
    "        print(\"https://docs-prv.pcisecuritystandards.org/PCI%20DSS/Supporting%20Document/Prioritized-Approach-Tool-For-PCI-DSS-v4_0_1.xlsx\")\n",
    "    else:\n",
    "        print(f\"Found: {excel_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_requirement_cell(text: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Parse requirement ID and title from Excel cell.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    \n",
    "    # Pattern: \"1.1.1 Title text\"\n",
    "    match = re.match(r'^(\\d+(?:\\.\\d+)*)\\s+(.+)', text)\n",
    "    if match:\n",
    "        req_id = match.group(1)\n",
    "        title = match.group(2).strip()\n",
    "        level = len(req_id.split('.'))\n",
    "        return {'id': req_id, 'title': title, 'level': level}\n",
    "    \n",
    "    # Pattern: \"Requirement 1: Title\"\n",
    "    section_match = re.match(r'^Requirement\\s+(\\d+):\\s*(.+)', text, re.IGNORECASE)\n",
    "    if section_match:\n",
    "        return {\n",
    "            'id': section_match.group(1),\n",
    "            'title': section_match.group(2).strip(),\n",
    "            'level': 1\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test\n",
    "test_cases = [\n",
    "    \"Requirement 1: Install and Maintain Network Security Controls\",\n",
    "    \"1.1 Processes and mechanisms for network security controls\",\n",
    "    \"1.2.3 Specific requirement text\"\n",
    "]\n",
    "print(\"Testing parser:\")\n",
    "for text in test_cases:\n",
    "    result = parse_requirement_cell(text)\n",
    "    if result:\n",
    "        print(f\"  {result['id']}: Level {result['level']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "import warnings\n",
    "\n",
    "# Suppress harmless openpyxl warning about Data Validation\n",
    "# (Excel has dropdown menus that openpyxl ignores - doesn't affect our data)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')\n",
    "\n",
    "def extract_requirements(excel_path: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract requirements from Excel file.\"\"\"\n",
    "    workbook = load_workbook(excel_path, data_only=True)\n",
    "    \n",
    "    sheet_name = \"Prioritized Approach Milestones\"\n",
    "    if sheet_name not in workbook.sheetnames:\n",
    "        print(f\"Available sheets: {workbook.sheetnames}\")\n",
    "        return []\n",
    "    \n",
    "    worksheet = workbook[sheet_name]\n",
    "    requirements = []\n",
    "    \n",
    "    for row in worksheet.iter_rows(min_row=3, values_only=True):\n",
    "        if not row or not row[0]:\n",
    "            continue\n",
    "        \n",
    "        parsed = parse_requirement_cell(str(row[0]))\n",
    "        if not parsed:\n",
    "            continue\n",
    "        \n",
    "        parts = parsed['id'].split('.')\n",
    "        parent_id = '.'.join(parts[:-1]) if len(parts) > 1 else None\n",
    "        \n",
    "        requirement = {\n",
    "            'id': parsed['id'],\n",
    "            'level': parsed['level'],\n",
    "            'parentId': parent_id,\n",
    "            'title': parsed['title'],\n",
    "            'statement': parsed['title'],\n",
    "        }\n",
    "        \n",
    "        if row[1]:\n",
    "            requirement['milestone'] = int(row[1])\n",
    "        \n",
    "        requirements.append(requirement)\n",
    "    \n",
    "    requirements.sort(key=lambda r: [int(x) for x in r['id'].split('.')])\n",
    "    return requirements\n",
    "\n",
    "# Extract requirements\n",
    "if excel_path.exists():\n",
    "    requirements = extract_requirements(excel_path)\n",
    "    print(f\"Extracted {len(requirements)} requirements\")\n",
    "    \n",
    "    levels = {}\n",
    "    for req in requirements:\n",
    "        levels[req['level']] = levels.get(req['level'], 0) + 1\n",
    "    \n",
    "    # Show level distribution with explanation\n",
    "    print(\"\\nLevel distribution:\")\n",
    "    level_names = {\n",
    "        1: \"Principal requirements (Requirement 1-12)\",\n",
    "        2: \"Sub-requirements (1.1, 1.2, etc.)\",\n",
    "        3: \"Detailed requirements (1.1.1, 1.2.3, etc.)\",\n",
    "        4: \"Specific controls (1.2.3.1, etc.)\",\n",
    "        5: \"Deep nesting (rare)\"\n",
    "    }\n",
    "    for level in sorted(levels.keys()):\n",
    "        desc = level_names.get(level, \"\")\n",
    "        print(f\"  Level {level}: {levels[level]:>3}  {desc}\")\n",
    "    \n",
    "    print(f\"\\nThis is the expected distribution for PCI-DSS v4.0.1\")\n",
    "else:\n",
    "    requirements = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building a Fast Index\n",
    "\n",
    "When an AI asks \"What is requirement 1.2.3?\", we need O(1) lookup, not O(n) linear search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequirementIndex:\n",
    "    \"\"\"Fast O(1) index for requirement lookups.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.requirements: Dict[str, Dict] = {}\n",
    "        self.by_level: Dict[int, List[str]] = {}\n",
    "        \n",
    "    def add(self, req: Dict):\n",
    "        self.requirements[req['id']] = req\n",
    "        level = req['level']\n",
    "        if level not in self.by_level:\n",
    "            self.by_level[level] = []\n",
    "        self.by_level[level].append(req['id'])\n",
    "    \n",
    "    def get(self, req_id: str) -> Optional[Dict]:\n",
    "        return self.requirements.get(req_id)\n",
    "    \n",
    "    def get_children(self, parent_id: str) -> List[Dict]:\n",
    "        return sorted(\n",
    "            [r for r in self.requirements.values() if r.get('parentId') == parent_id],\n",
    "            key=lambda r: [int(x) for x in r['id'].split('.')]\n",
    "        )\n",
    "\n",
    "# Build index\n",
    "if requirements:\n",
    "    index = RequirementIndex()\n",
    "    for req in requirements:\n",
    "        index.add(req)\n",
    "    print(f\"Indexed {len(index.requirements)} requirements\")\n",
    "    \n",
    "    # Test\n",
    "    req = index.get(\"1.2\")\n",
    "    if req:\n",
    "        print(f\"\\nTest lookup 1.2: {req['title'][:50]}...\")\n",
    "        children = index.get_children(\"1.2\")\n",
    "        print(f\"Children of 1.2: {len(children)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: MCP Tool Patterns (Python)\n",
    "\n",
    "Every MCP tool follows: **Schema -> Execute -> Return**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GetRequirementInput:\n",
    "    id: str\n",
    "    include_children: bool = False\n",
    "\n",
    "def get_requirement_tool(input: GetRequirementInput, idx: RequirementIndex):\n",
    "    \"\"\"MCP tool pattern: validate -> execute -> return\"\"\"\n",
    "    req = idx.get(input.id)\n",
    "    if not req:\n",
    "        raise ValueError(f\"Requirement not found: {input.id}\")\n",
    "    \n",
    "    result = {'requirement': req}\n",
    "    if input.include_children:\n",
    "        result['children'] = idx.get_children(input.id)\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "if requirements:\n",
    "    result = get_requirement_tool(GetRequirementInput(id=\"1\", include_children=True), index)\n",
    "    print(f\"Requirement 1: {result['requirement']['title'][:40]}...\")\n",
    "    print(f\"Children: {len(result['children'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Full-Text Search\n",
    "\n",
    "TF-IDF scoring for \"find requirements about encryption\" queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class SearchIndex:\n",
    "    \"\"\"Simple TF-IDF search.\"\"\"\n",
    "    \n",
    "    def __init__(self, reqs: List[Dict]):\n",
    "        self.requirements = {r['id']: r for r in reqs}\n",
    "        self.inverted_index: Dict[str, List] = {}\n",
    "        self.doc_count = len(reqs)\n",
    "        \n",
    "        for req in reqs:\n",
    "            text = f\"{req['id']} {req['title']}\".lower()\n",
    "            word_counts = Counter(text.split())\n",
    "            for word, count in word_counts.items():\n",
    "                if word not in self.inverted_index:\n",
    "                    self.inverted_index[word] = []\n",
    "                self.inverted_index[word].append((req['id'], count))\n",
    "    \n",
    "    def search(self, query: str, limit: int = 5):\n",
    "        scores = {}\n",
    "        for word in query.lower().split():\n",
    "            if word not in self.inverted_index:\n",
    "                continue\n",
    "            idf = math.log(self.doc_count / len(self.inverted_index[word]))\n",
    "            for doc_id, tf in self.inverted_index[word]:\n",
    "                scores[doc_id] = scores.get(doc_id, 0) + tf * idf\n",
    "        \n",
    "        return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:limit]\n",
    "\n",
    "# Test\n",
    "if requirements:\n",
    "    search_index = SearchIndex(requirements)\n",
    "    results = search_index.search(\"network security firewall\")\n",
    "    print(\"Search: 'network security firewall'\")\n",
    "    for req_id, score in results:\n",
    "        print(f\"  [{req_id}] {search_index.requirements[req_id]['title'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Save Data as JSON\n",
    "\n",
    "Export for use in the MCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom datetime import datetime\n\nif requirements:\n    output = {\n        'info': {\n            'version': '4.0.1',\n            'convertedAt': datetime.now().isoformat(),\n        },\n        'requirements': requirements,\n    }\n    \n    output_file = Path('requirements.json')\n    with open(output_file, 'w') as f:\n        json.dump(output, f, indent=2)\n    \n    print(f\"Saved: {output_file} ({output_file.stat().st_size / 1024:.1f} KB)\")\n    print(f\"Requirements: {len(requirements)}\")\n    \n    if IN_COLAB:\n        print(\"\\n(File will be used by the FastMCP server in Part 8)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Adapting for Other Frameworks\n",
    "\n",
    "The same patterns work for any compliance framework:\n",
    "\n",
    "### NIST 800-53\n",
    "```python\n",
    "# Pattern: AC-1, AC-1(1)\n",
    "match = re.match(r'^([A-Z]{2}-\\d+(?:\\(\\d+\\))?)\\s+(.+)$', text)\n",
    "```\n",
    "\n",
    "### ISO 27001\n",
    "```python\n",
    "# Pattern: A.5.1.1\n",
    "match = re.match(r'^(A\\.\\d+(?:\\.\\d+)*)\\s+(.+)$', text)\n",
    "```\n",
    "\n",
    "### Key Steps\n",
    "1. Find structured data (Excel/CSV > PDF)\n",
    "2. Write regex for your ID format\n",
    "3. Build the same index structure\n",
    "4. Adapt tool descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 8: Building the MCP Server with FastMCP\n\nNow let's build a **real, working MCP server** using [FastMCP](https://github.com/jlowin/fastmcp) - the leading Python framework for MCP development.\n\n### Why FastMCP?\n- Pure Python - no TypeScript/Node.js needed\n- Decorator-based API - clean and intuitive\n- Production-ready - used by many organizations\n- Great documentation and community support"
  },
  {
   "cell_type": "code",
   "source": "# Install FastMCP\n!pip install -q fastmcp\nprint(\"FastMCP installed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "%%writefile compliance_server.py\n\"\"\"\nPCI-DSS Compliance MCP Server\nBuilt with FastMCP - the leading Python framework for MCP development\n\"\"\"\nfrom fastmcp import FastMCP\nimport json\nfrom pathlib import Path\n\n# Initialize server\nmcp = FastMCP(\"pci-dss-compliance\")\n\n# Load requirements at startup\ndata = json.loads(Path(\"requirements.json\").read_text())\nREQUIREMENTS = {r[\"id\"]: r for r in data[\"requirements\"]}\n\n@mcp.tool()\ndef get_requirement(id: str, include_children: bool = False) -> dict:\n    \"\"\"\n    Get a specific PCI-DSS requirement by ID.\n\n    Args:\n        id: Requirement ID (e.g., \"1.2.3\")\n        include_children: Include child requirements in response\n    \"\"\"\n    req = REQUIREMENTS.get(id)\n    if not req:\n        return {\"error\": f\"Requirement {id} not found\"}\n\n    result = {\"requirement\": req}\n    if include_children:\n        result[\"children\"] = [\n            r for r in REQUIREMENTS.values()\n            if r.get(\"parentId\") == id\n        ]\n    return result\n\n@mcp.tool()\ndef search_requirements(query: str, limit: int = 10) -> dict:\n    \"\"\"\n    Search PCI-DSS requirements by keyword.\n\n    Args:\n        query: Search terms (e.g., \"encryption\", \"firewall\")\n        limit: Maximum number of results to return\n    \"\"\"\n    query_lower = query.lower()\n    matches = [\n        {\"id\": r[\"id\"], \"title\": r[\"title\"]}\n        for r in REQUIREMENTS.values()\n        if query_lower in r[\"title\"].lower()\n    ]\n    return {\n        \"query\": query,\n        \"count\": len(matches),\n        \"results\": matches[:limit]\n    }\n\n@mcp.tool()\ndef list_requirements(level: int = None, limit: int = 20) -> dict:\n    \"\"\"\n    List PCI-DSS requirements, optionally filtered by hierarchy level.\n\n    Args:\n        level: Filter by level (1=principal, 2=sub, 3=detailed, 4=specific)\n        limit: Maximum number of results to return\n    \"\"\"\n    if level:\n        reqs = [r for r in REQUIREMENTS.values() if r[\"level\"] == level]\n    else:\n        reqs = list(REQUIREMENTS.values())\n\n    return {\n        \"level\": level or \"all\",\n        \"count\": len(reqs),\n        \"requirements\": [\n            {\"id\": r[\"id\"], \"title\": r[\"title\"]}\n            for r in reqs[:limit]\n        ]\n    }\n\nif __name__ == \"__main__\":\n    mcp.run()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test the MCP server logic directly\n# Note: FastMCP decorators wrap functions for the MCP protocol,\n# so we test the logic by re-implementing it here with our loaded data.\n\nimport json\n\n# Load the JSON data (same as the server does)\nwith open(\"requirements.json\") as f:\n    data = json.load(f)\nREQUIREMENTS = {r[\"id\"]: r for r in data[\"requirements\"]}\n\nprint(\"=== Testing MCP Server Logic ===\\n\")\n\n# Test get_requirement logic\nprint(\"--- get_requirement('1', include_children=True) ---\")\nreq = REQUIREMENTS.get(\"1\")\nresult = {\"requirement\": req}\nresult[\"children\"] = [r for r in REQUIREMENTS.values() if r.get(\"parentId\") == \"1\"]\nprint(f\"Requirement: {result['requirement']['title']}\")\nprint(f\"Children: {len(result['children'])} sub-requirements\\n\")\n\n# Test search_requirements logic\nprint(\"--- search_requirements('network security') ---\")\nquery_lower = \"network security\".lower()\nmatches = [\n    {\"id\": r[\"id\"], \"title\": r[\"title\"]}\n    for r in REQUIREMENTS.values()\n    if query_lower in r[\"title\"].lower()\n]\nprint(f\"Found {len(matches)} matches:\")\nfor r in matches[:3]:\n    print(f\"  [{r['id']}] {r['title'][:50]}...\")\nprint()\n\n# Test list_requirements logic\nprint(\"--- list_requirements(level=1) ---\")\nlevel_1 = [r for r in REQUIREMENTS.values() if r[\"level\"] == 1]\nprint(f\"Level 1 requirements: {len(level_1)}\")\nfor r in level_1[:5]:\n    print(f\"  [{r['id']}] {r['title'][:50]}...\")\n\nprint(\"\\n=== Server Logic Tests Complete ===\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 9: Security Considerations\n\nCompliance data is sensitive. Follow these guidelines when deploying MCP servers:\n\n### Deploy Inside Your Firewall\n- Run MCP servers on internal networks only\n- Never expose compliance data to public endpoints\n- Use VPN or internal-only access for remote teams\n\n### Least Privilege Access\n- Only expose read operations (no writes unless required)\n- Don't include admin or configuration tools\n- Limit to the minimum data required for the use case\n\n### Data Handling\n- Never log full requirement content in production\n- Be careful with PII in audit evidence\n- Follow your organization's data classification policies\n\n### Security is Evolving\n\n> \"Security's still the Achilles heel of MCPs and LLMs\"\n> — [The Pragmatic Engineer](https://newsletter.pragmaticengineer.com/p/mcp-deepdive)\n\nBest practices are still emerging. Stay updated:\n- [MCP Security Guidelines](https://modelcontextprotocol.io/docs/concepts/security)\n- Your organization's AI/ML security policies",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Part 10: Testing & Debugging\n\n### MCP Inspector\n\nThe official debugging tool for MCP servers:\n\n```bash\nnpx @modelcontextprotocol/inspector python compliance_server.py\n```\n\nThis opens a web UI where you can:\n- See all available tools\n- Test tool calls interactively\n- View request/response payloads\n\n### Claude Desktop Integration\n\nTo use your server with Claude Desktop, add to `~/.claude/claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"pci-dss\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/compliance_server.py\"]\n    }\n  }\n}\n```\n\nThen restart Claude Desktop. Your tools will appear in Claude's tool list.\n\n### Testing Checklist\n\nBefore deploying:\n- [ ] Server starts without errors\n- [ ] All tools return valid JSON\n- [ ] Error cases return helpful messages\n- [ ] Performance is acceptable (<1s for lookups)\n- [ ] Works in MCP Inspector\n- [ ] Works in Claude Desktop",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Summary\n\nYou've learned to build MCP servers for compliance documentation using Python:\n\n1. **Data Extraction** - Excel parsing with openpyxl (more reliable than PDFs)\n2. **Indexing** - O(1) dictionary lookups for fast retrieval\n3. **MCP Tool Patterns** - Schema → Execute → Return\n4. **Full-Text Search** - TF-IDF scoring for keyword queries\n5. **FastMCP Server** - Production-ready Python MCP server\n6. **Security** - Internal deployment, least privilege access\n7. **Testing** - MCP Inspector and Claude Desktop integration\n\n### What's Next?\n\nIntegrate your compliance MCP with other systems:\n- **Jira** - Link requirements to implementation tickets\n- **GitHub** - Connect to evidence repositories\n- **Confluence** - Pull policy documentation\n- **Datadog/Sentry** - Correlate with monitoring alerts\n\n### Resources\n\n- [FastMCP Documentation](https://github.com/jlowin/fastmcp)\n- [MCP Specification](https://modelcontextprotocol.io/specification)\n- [MCP Inspector](https://github.com/modelcontextprotocol/inspector)\n- [The Pragmatic Engineer - MCP Deep Dive](https://newsletter.pragmaticengineer.com/p/mcp-deepdive)\n- [PCI-DSS v4.0.1 Documentation](https://www.pcisecuritystandards.org/document_library/)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}